{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Wews_jq-UDA",
        "outputId": "7b95a807-a750-4e14-8981-e78ba52d6fc6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.12.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
        "import numpy as np\n",
        "import os\n",
        "\n"
      ],
      "metadata": {
        "id": "DgxqYFaj-gU1"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training function\n",
        "def trainingFunction():\n",
        "    print(\"Training under process...\")\n",
        "    #basic cnn\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128,128, 3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F5VE9mRR-008"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Setup image generators\n",
        "train_datagen = ImageDataGenerator(shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/TrainingDataset',\n",
        "                                                     target_size=(128, 128),\n",
        "                                                     batch_size=8,\n",
        "                                                     class_mode='categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/TestingDataset',\n",
        "                                                target_size=(128, 128),\n",
        "                                                batch_size=8,\n",
        "                                                class_mode='categorical')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhSAuKsL-6cC",
        "outputId": "efe95d2e-e9d8-448a-ea97-3bbadde638b6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 50 images belonging to 2 classes.\n",
            "Found 50 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training function\n",
        "def trainingFunction():\n",
        "    print(\"Training under process...\")\n",
        "    #basic cnn\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128,128, 3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Setup image generators\n",
        "    train_datagen = ImageDataGenerator(shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/TrainingDataset',\n",
        "                                                         target_size=(128, 128),\n",
        "                                                         batch_size=8,\n",
        "                                                         class_mode='categorical')\n",
        "\n",
        "    test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/TestingDataset',\n",
        "                                                    target_size=(128, 128),\n",
        "                                                    batch_size=8,\n",
        "                                                    class_mode='categorical')\n",
        "\n",
        "    model.fit(training_set,\n",
        "              steps_per_epoch=100,\n",
        "              epochs=10,\n",
        "              validation_data=test_set,\n",
        "              validation_steps=125)\n",
        "\n",
        "    # Save model\n",
        "    model_json = model.to_json()\n",
        "    with open(\"model.json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "        model.save_weights(\"model.weights.h5\")\n",
        "        print(\"Saved model to disk\") #This line and the lines above need to be indented\n",
        "trainingFunction()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hafJpBUNCBGv",
        "outputId": "468cfb20-752b-493a-c7e4-3576b4a08b30"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training under process...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 50 images belonging to 2 classes.\n",
            "Found 50 images belonging to 2 classes.\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m  7/100\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 223ms/step - accuracy: 0.5497 - loss: 0.8061"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - accuracy: 0.6337 - loss: 0.6989 - val_accuracy: 0.5000 - val_loss: 0.6941\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.8358 - loss: 0.3571 - val_accuracy: 0.5000 - val_loss: 0.6990\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 27ms/step - accuracy: 0.8620 - loss: 0.3222 - val_accuracy: 0.5000 - val_loss: 0.7137\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 37ms/step - accuracy: 0.8623 - loss: 0.2766 - val_accuracy: 0.5000 - val_loss: 0.7059\n",
            "Epoch 5/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.9163 - loss: 0.2226 - val_accuracy: 0.5000 - val_loss: 0.6959\n",
            "Epoch 6/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.9189 - loss: 0.2144 - val_accuracy: 0.5000 - val_loss: 0.6918\n",
            "Epoch 7/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - accuracy: 0.9619 - loss: 0.1047 - val_accuracy: 0.5000 - val_loss: 0.6945\n",
            "Epoch 8/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.9191 - loss: 0.1813 - val_accuracy: 0.5000 - val_loss: 0.9403\n",
            "Epoch 9/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.9218 - loss: 0.1671 - val_accuracy: 0.5000 - val_loss: 0.9751\n",
            "Epoch 10/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.0759 - val_accuracy: 0.5000 - val_loss: 0.6970\n",
            "Saved model to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fine the classify function\n",
        "def classifyFunction(image_path):\n",
        "    # Load model\n",
        "    json_file = open('model.json', 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json)\n",
        "    loaded_model.load_weights(\"model.weights.h5\")\n",
        "\n",
        "    # Define the labels\n",
        "    labels = [\"Covid\", \"Normal\"]\n",
        "\n",
        "    # Load and process image\n",
        "    test_image = image.load_img(image_path, target_size=(128, 128))\n",
        "    test_image = image.img_to_array(test_image)\n",
        "    test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "    # Predict\n",
        "    result = loaded_model.predict(test_image)\n",
        "    label_index = result.argmax()\n",
        "    print(f\"Predicted label: {labels[label_index]}\")\n"
      ],
      "metadata": {
        "id": "Ee5XAbsiCMtj"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "trainingFunction() # Uncomment to train the model\n",
        "classifyFunction('/content/drive/MyDrive/TestingDataset/Covid/1-s2.0-S0140673620303706-fx1_lrg.jpg') # Uncomment to classify an image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laCScojBDV1P",
        "outputId": "0f26cad5-a76a-4f60-9586-3732c9a22c0d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training under process...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step\n",
            "Predicted label: Covid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N5-_PAmjI2AF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}